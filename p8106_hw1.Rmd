---
title: "P8106 HW1" 
author: "Shihui Zhu"
output:
  pdf_document:
    toc: yes
    toc_depth: 2
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '2'
header-includes:
- \usepackage{fancyhdr}
- \usepackage{lipsum}
- \pagestyle{fancy}
- \fancyhead[R]{\thepage}
- \fancypagestyle{plain}{\pagestyle{fancy}}
--- 

```{r setup, include=FALSE}
# This chunk loads all the packages used in this homework
library(tidyverse)
library(corrplot)
library(glmnet)
#library(RNHANES)
library(leaps)
library(plotmo)
library(caret)
#library(FNN) # knn.reg()
#library(doBy) # which.minn()

# General figure set up
knitr::opts_chunk$set(
  # display the code in github doc
  echo = TRUE,
  # hide warning messages
  warning = FALSE,
  # set the figure to be 8 x 6, and the proportion it takes to be 95%
  fig.width = 8,
  fig.height = 6, 
  out.width = "95%"
)
```

# (a) Least Square

Fit a linear model using least squares on the training data. Is there any potential disadvantage of this model?
```{r input_train}
house_training <- read_csv("housing_training.csv") %>%
  janitor::clean_names()
```

## Perform 10-folds cross-validation on the full model

There are 25 predictors in total, we want to perform CV to find the best fitted parameters. 

```{r}
# Reproducibility
set.seed(1)
fit.lm <- train(sale_price ~ ., 
                data = house_training,
                method = "lm",
                trControl = trainControl(method = "cv", number = 10))
# Print the coefficients of the final model
fit.lm$finalModel$coefficients
```

## MSE, R-squared values

Check the mean squared error (MSE) and the R-squared value of the model:
```{r rmse}
# The Mean Squared Error is
mean((fit.lm$resample$RMSE)^2)
# The R^2 value is
mean(fit.lm$resample$Rsquared)
```

So the training error for the LS model is 515636300, with R-squared value of 0.9029694.

Potential disadvantage:
1. Overfit
2. ?The true regression is never linear

# (b) Lasso

```{r corr}
x_train <- model.matrix(sale_price ~ ., house_training)[ ,-1]
y_train <- house_training$sale_price

# Correlation plot
corrplot(cor(x_train), method = "circle", type = "full")
```

## Fit the model using Lasso Regression

```{r lasso}
set.seed(1)
# fit the laso regression (alpha = 1) with a sequence of lambdas
cv.lasso <- cv.glmnet(x = x_train, y = y_train, 
                    standardize = TRUE,
                    alpha = 1, 
                    lambda = exp(seq(8, -1, length = 100)))
```


```{r plot}
plot(cv.lasso)
abline(h = (cv.lasso$cvm + cv.lasso$cvsd)[which.min(cv.lasso$cvm)], col = 4, lwd = 2)
```

When 1se rule is applied, there are total 30 predictors in the model. 

```{r coeff}
predict(cv.lasso, s = cv.lasso$lambda.1se, type = "coefficient")
```

## Test Error

Calculate the MSE of the test set

```{r test_lasso}
housing_testing <- read_csv("housing_test.csv") %>%
  janitor::clean_names()
x_test <- model.matrix(sale_price ~ ., housing_testing)[ ,-1]
y_test <- housing_testing$sale_price

# Training Error
y_pred_t <- predict(cv.lasso, newx = x_train, s = "lambda.1se", type = "response")
mean(RMSE(y_pred_t, y_train)^2)
```

The training error (MSE) is 515636300 for lasso model.

```{r}
y_pred <- predict(cv.lasso, newx = x_test, s = "lambda.1se", type = "response")
mean(RMSE(y_pred, y_test)^2)
```

The test error (MSE) is 420354616 for lasso regression model when the 1SE rule is applied.

# c) Fit an elastic net model on the training data

## Report the selected tuning parameters

```{r elastic}
set.seed(2)
enet.fit <- train(x_train, y_train,
                  method = "glmnet",
                  tuneGrid = expand.grid(alpha = seq(0, 1, length = 21), 
                                         lambda = exp(seq(8, -3, length = 50))),
                  trControl = trainControl(method = "repeatedcv", number = 10, repeats = 5))
enet.fit$bestTune

# Set rainbow color
myCol<- rainbow(25)
myPar <- list(superpose.symbol = list(col = myCol),
                    superpose.line = list(col = myCol))

plot(enet.fit, par.settings = myPar)

coef(enet.fit$finalModel, enet.fit$bestTune$lambda)
```

The tunning parameter $\lambda$ is 619.2886. The parameter $\alpha$ is 0.05. 

## Test error

```{r}
enet.pred <- predict(enet.fit, newdata = x_test)
# test error
mean(RMSE(enet.pred, y_test)^2)
```

# (d) Fit a partial least squares model 

```{r}

```



```{r}
set.seed(2)

resamp <- resamples(list(enet = enet.fit, lasso = lasso.fit, ridge = ridge.fit, lm = lm.fit))
summary(resamp)

parallelplot(resamp, metric = "RMSE")
# bwplot(resamp, metric = "RMSE")
```

