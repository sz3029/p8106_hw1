---
title: "P8106 HW1" 
author: "Shihui Zhu"
output:
  pdf_document:
    toc: yes
    toc_depth: 2
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '2'
header-includes:
- \usepackage{fancyhdr}
- \usepackage{lipsum}
- \pagestyle{fancy}
- \fancyhead[R]{\thepage}
- \fancypagestyle{plain}{\pagestyle{fancy}}
--- 

```{r setup, include=FALSE}
# This chunk loads all the packages used in this homework
library(tidyverse)
library(corrplot)
library(glmnet) # for lasso and elastic net
#library(RNHANES)
library(leaps)
library(plotmo)
library(caret)
library(pls) # for pls

# General figure set up
knitr::opts_chunk$set(
  # display the code in github doc
  echo = TRUE,
  # hide warning messages
  warning = FALSE
)
```

# (a) Least Square

Fit a linear model using least squares on the training data. Is there any potential disadvantage of this model?
```{r input_train}
house_training <- read_csv("housing_training.csv") %>%
  janitor::clean_names()
```

## Perform 10-folds cross-validation on the full model

There are 25 predictors in total, we want to perform CV to find the best fitted parameters. 

```{r, warning=FALSE}
# Reproducibility
set.seed(1)
fit.lm <- train(sale_price ~ ., 
                data = house_training,
                method = "lm",
                trControl = trainControl(method = "cv", number = 10))
# Print the coefficients of the final model
summary(fit.lm)
```

## MSE, R-squared values

Check the mean squared error (MSE) and the R-squared value of the model:
```{r rmse}
# The Mean Squared Error is
mean((fit.lm$resample$RMSE)^2)
# The R^2 value is
mean(fit.lm$resample$Rsquared)
```

So the training error for the LS model is 531169999, with adjusted R-squared value of 0.9029694.

Potential disadvantage:
1. The model has the smallest variance among all the unbiased models, but it has larger variance compared to biased models, but it is not necessarily good enough. It may be better to have a biased estimator with smaller variance/mean squared error. 
2. There are many predictors used in this model, and this can cause problem (large variance, colinearity among predictors, interpretation becomes hazardous).
3. The model is too complex, a simple model might be better.
4. This model has overfitting problem, meaning that it may perform badly on testing set. 

# (b) Lasso

```{r corr}
x_train <- model.matrix(sale_price ~ ., house_training)[ ,-1]
y_train <- house_training$sale_price
```

## Fit the model using Lasso Regression

```{r lasso}
set.seed(1)
# fit the laso regression (alpha = 1) with a sequence of lambdas
cv.lasso <- cv.glmnet(x = x_train, y = y_train, 
                    standardize = TRUE,
                    alpha = 1, 
                    lambda = exp(seq(8, -1, length = 100)))
```


```{r plot}
plot(cv.lasso)
abline(h = (cv.lasso$cvm + cv.lasso$cvsd)[which.min(cv.lasso$cvm)], col = 4, lwd = 2)
```

When 1se rule is applied, there are total 30 predictors in the model. 

The coefficient of LASSO regression model with 1SE rule applied is:

```{r coeff}
predict(cv.lasso, s = cv.lasso$lambda.1se, type = "coefficient")
```

## Training and Testing Error

Calculate the MSE of the test set

```{r test_lasso, message=FALSE}
housing_testing <- read_csv("housing_test.csv") %>%
  janitor::clean_names()
x_test <- model.matrix(sale_price ~ ., housing_testing)[ ,-1]
y_test <- housing_testing$sale_price

# Training Error
y_pred_t <- predict(cv.lasso, newx = x_train, s = "lambda.1se", type = "response")
mean(RMSE(y_pred_t, y_train)^2)
```

The training error (MSE) is 515636300 for lasso model.

```{r}
y_pred <- predict(cv.lasso, newx = x_test, s = "lambda.1se", type = "response")
lasso_te <- mean(RMSE(y_pred, y_test)^2)
lasso_te
```

The test error (MSE) is 420354616 for lasso regression model when the 1SE rule is applied.

# c) Fit an elastic net model on the training data

## Report the selected tuning parameters

```{r elastic}
set.seed(2)
enet.fit <- train(x_train, y_train,
                  method = "glmnet",
                  tuneGrid = expand.grid(alpha = seq(0, 1, length = 21), 
                                         lambda = exp(seq(8, -3, length = 50))),
                  trControl = trainControl(method = "repeatedcv", number = 10, repeats = 5))
enet.fit$bestTune

# Set rainbow color
myCol<- rainbow(25)
myPar <- list(superpose.symbol = list(col = myCol),
                    superpose.line = list(col = myCol))

plot(enet.fit, par.settings = myPar)

coef(enet.fit$finalModel, enet.fit$bestTune$lambda)
```

The tunning parameter $\lambda$ is 619.2886. The parameter $\alpha$ is 0.05. 

## Test error

```{r}
enet.pred <- predict(enet.fit, newdata = x_test)
# test error
enet_te <- mean(RMSE(enet.pred, y_test)^2)
enet_te
```

The test error (MSE) is 438209306 for elastic net model.

# (d) Fit a partial least squares model 

## Fit model using plsr
```{r}
set.seed(2)
pls.mod <- plsr(sale_price~., 
                data = house_training, 
                scale = TRUE,  
                validation = "CV")

summary(pls.mod)
validationplot(pls.mod, val.type="MSEP", legendpos = "topright")
```
## Training Error and Testing Error
```{r}
# training error
cv.mse <- RMSEP(pls.mod)
mean(min(cv.mse$val[1,,])^2)

ncomp.cv <- which.min(cv.mse$val[1,,])-1 # extract the response and delete the 0th component
# num of components
ncomp.cv

# Prediction
pls_pred <- predict(pls.mod, newdata = x_test, 
                      ncomp = ncomp.cv)
# test MSE
pls_te <- mean(RMSE(y_test, pls_pred)^2)
pls_te
```

The model with 8 components yields the minimum training error (MSE) 529300397. The testing error (MSE) is 440217938.

# e) Select Model

```{r}
#compute the testing error of LS regression first
y_pred <- predict(fit.lm, newdata = housing_testing)
ls_te <- mean(RMSE(y_pred, y_test)^2)
ls_te
```

The test error of LS model is 447287652. 

For response predicting, we want to choose the model with the smallest test error. The test error (MSE) is 420354616 for lasso regression model, 438209306 for elastic net model, and 440217938 for the PLS model. Therefore, we should choose the lasso regression model for predicting the response. 

